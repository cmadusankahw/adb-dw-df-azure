{
	"name": "Dim-transformations",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "adbsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "eb900b26-e584-453d-917c-cc861320305c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/03e69755-ef75-48a3-b96f-506247a595b8/resourceGroups/azure-ceritifications-training/providers/Microsoft.Synapse/workspaces/adb-dw-dm-synapse/bigDataPools/adbsparkpool",
				"name": "adbsparkpool",
				"type": "Spark",
				"endpoint": "https://adb-dw-dm-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/adbsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.types import *\r\n",
					"from pyspark.sql.functions import *\r\n",
					"from pyspark.sql.functions import udf\r\n",
					"from pyspark.sql.utils import AnalysisException\r\n",
					"from pyspark.sql import Row\r\n",
					"import datetime\r\n",
					"import ast\r\n",
					"import re\r\n",
					"import calendar\r\n",
					"import pandas\r\n",
					"import holidays"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"account_name = \"chiransubdl\"\r\n",
					"container_name = \"chiransubfs\"\r\n",
					"\r\n",
					"customer_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/customers.csv\")\r\n",
					"geolocation_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/geolocation.csv\")\r\n",
					"delivery_services_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/delivery_services.csv\")\r\n",
					"order_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/orders.csv\")\r\n",
					"order_items_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/order_items.csv\")\r\n",
					"order_reviews_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/order_reviews.csv\")\r\n",
					"order_payments_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/order_payments.csv\")\r\n",
					"products_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/products.csv\")\r\n",
					"product_categories_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/product_categories.csv\")\r\n",
					"sellers_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/sellers.csv\")\r\n",
					"suppliers_raw_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/stage/suppliers.csv\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"customer_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(customer_raw_path)\r\n",
					"customer_df.createOrReplaceTempView(\"customer\")\r\n",
					"\r\n",
					"order_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(order_raw_path)\r\n",
					"order_df.createOrReplaceTempView(\"order\")\r\n",
					"\r\n",
					"order_items_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(order_items_raw_path)\r\n",
					"order_items_df.createOrReplaceTempView(\"orderitems\")\r\n",
					"\r\n",
					"order_payments_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(order_payments_raw_path)\r\n",
					"order_payments_df.createOrReplaceTempView(\"orderpayments\")\r\n",
					"\r\n",
					"order_reviews_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(order_reviews_raw_path)\r\n",
					"order_reviews_df.createOrReplaceTempView(\"orderreviews\")\r\n",
					"\r\n",
					"products_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(products_raw_path)\r\n",
					"products_df.createOrReplaceTempView(\"product\")\r\n",
					"\r\n",
					"product_categorie_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(product_categories_raw_path)\r\n",
					"product_categorie_df.createOrReplaceTempView(\"productcategories\")\r\n",
					"\r\n",
					"geolocation_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(geolocation_raw_path)\r\n",
					"geolocation_df.createOrReplaceTempView(\"geolocation\")\r\n",
					"\r\n",
					"delivery_services_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(delivery_services_raw_path)\r\n",
					"delivery_services_df.createOrReplaceTempView(\"deliveryservices\")\r\n",
					"\r\n",
					"seller_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(sellers_raw_path)\r\n",
					"seller_df.createOrReplaceTempView(\"seller\")\r\n",
					"\r\n",
					"supplier_df = spark.read\\\r\n",
					"                .option(\"header\", True)\\\r\n",
					"                .option(\"inferSchema\", True)\\\r\n",
					"                .csv(suppliers_raw_path)\r\n",
					"supplier_df.createOrReplaceTempView(\"supplier\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Generic UDF Definition"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": true,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\"\"\"\r\n",
					"    Generic UDF to generate surrogate key for a table\r\n",
					"\"\"\"\r\n",
					"def sk_generator(keycol,colname,df):\r\n",
					"    return spark.sql('select row_number() over (order by \"{}\") as {}, * from {}'.format(keycol,colname,df))\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get a substring of a given column\r\n",
					"\"\"\"\r\n",
					"def getsubstring(col,start,end):\r\n",
					"    try:\r\n",
					"        return str(col)[start:end]\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get quarter from date\r\n",
					"\"\"\"\r\n",
					"def getquarter(col,start,end):\r\n",
					"    try:\r\n",
					"        month = int(col[start:end])\r\n",
					"        if month > 0 and month < 4:\r\n",
					"            return 1\r\n",
					"        elif month > 3 and month < 7:\r\n",
					"            return 2\r\n",
					"        elif month >6 and month < 10:\r\n",
					"            return 3\r\n",
					"        else:\r\n",
					"            return 4\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"        \r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to calculate Age\r\n",
					"\"\"\"\r\n",
					"def calc_age(col):\r\n",
					"    cur_year = int(datetime.datetime.now().date().strftime(\"%Y\"))\r\n",
					"    try:\r\n",
					"        return (cur_year - int(col[0:4]))\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to bucketize given column values into a given no of buckets\r\n",
					"\"\"\"\r\n",
					"def bucketize(col,bucket_margins,bucket_names):\r\n",
					"    try:\r\n",
					"        for i in range(len(bucket_margins)):\r\n",
					"            if col < bucket_margins[i]:\r\n",
					"                return bucket_names[i]\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to bucketize non-numeric columns\r\n",
					"\"\"\"\r\n",
					"def social_state(col,bucket_values,bucket_names):\r\n",
					"    try:\r\n",
					"        for i in range(len(bucket_values)):\r\n",
					"            if col == bucket_values[i]:\r\n",
					"                return bucket_names[i]\r\n",
					"        else:\r\n",
					"            return bucket_names[-1]\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to set seller size\r\n",
					"\"\"\"\r\n",
					"def check_seller_size(col,limits):\r\n",
					"    try:\r\n",
					"        if int(col)> limits[0]:\r\n",
					"            return \"Small-Scale\"\r\n",
					"        if int(col)> limits[1]:\r\n",
					"            return \"Medium-Scale\"\r\n",
					"        if int(col)> limits[2]:\r\n",
					"            return \"Mass-Scale\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get unque list\r\n",
					"\"\"\"\r\n",
					"def get_unique_cat(col):\r\n",
					"    try:\r\n",
					"        return str(list(set(col)))\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get review sentiment\r\n",
					"\"\"\"\r\n",
					"def get_review_sentiment(col):\r\n",
					"    try:\r\n",
					"        if int(col)>3:\r\n",
					"            return \"Very Satisfied\"\r\n",
					"        elif int(col)==3:\r\n",
					"            return \"Moderate\"\r\n",
					"        elif int(col)<3:\r\n",
					"            return \"Not Satisfied\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get product volume\r\n",
					"\"\"\"\r\n",
					"def get_product_volume(col_arr):\r\n",
					"    try:\r\n",
					"        return float(col_arr[0]) * float(col_arr[1]) * float(col_arr[2])\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to get package type\r\n",
					"\"\"\"\r\n",
					"def get_package_type(col_arr):\r\n",
					"    try:\r\n",
					"        if float(col_arr[0]) < 3000 or float(col_arr[1])< 500:\r\n",
					"            return \"Small\"\r\n",
					"        if float(col_arr[0]) < 6000 or float(col_arr[1])< 1000:\r\n",
					"            return \"Medium\"\r\n",
					"        if float(col_arr[0]) > 5999 or float(col_arr[1])> 999:\r\n",
					"            return \"Large\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to check discount\r\n",
					"\"\"\"\r\n",
					"def check_discount(col):\r\n",
					"    try:\r\n",
					"        if int(col)> 0:\r\n",
					"            return \"YES\"\r\n",
					"        else:\r\n",
					"            return \"NO\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\"\"\"\r\n",
					"    Generic UDF to check delayed deliveries\r\n",
					"\"\"\"\r\n",
					"def chekc_delayed_delivery(col_arr):\r\n",
					"    try:\r\n",
					"        est_month = int((col_arr[0])[5:7])\r\n",
					"        est_date  = int((col_arr[0])[8:10])\r\n",
					"        act_month  = int((col_arr[1])[5:7])\r\n",
					"        act_date  = int((col_arr[1])[8:10])\r\n",
					"        if act_month > est_month:\r\n",
					"            return \"YES\"\r\n",
					"        elif act_date> est_date:\r\n",
					"            return \"YES\"\r\n",
					"        else:\r\n",
					"            return \"NO\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getweekday(col):\r\n",
					"    try:\r\n",
					"        year = int(col[0:4])\r\n",
					"        month = int(col[5:7])\r\n",
					"        day = int(col[8:10])\r\n",
					"        d1 = datetime.datetime(year,month,day,0,0,0,0)\r\n",
					"        return d1.isoweekday()\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getweekyear(col):\r\n",
					"    try:\r\n",
					"        year = int(col[0:4])\r\n",
					"        month = int(col[5:7])\r\n",
					"        day = int(col[8:10])\r\n",
					"        d1 = datetime.datetime(year,month,day,0,0,0,0)\r\n",
					"        return d1.isocalendar()[1]\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getleapyear(col):\r\n",
					"    try:\r\n",
					"        year = int(col[0:4])\r\n",
					"        month = int(col[5:7])\r\n",
					"        day = int(col[8:10])\r\n",
					"        d1 = datetime.datetime(year,month,day,0,0,0,0)\r\n",
					"        if calendar.isleap(d1.year):\r\n",
					"            return 'YES'\r\n",
					"        else:\r\n",
					"            return 'NO'\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getdayprefix(col):\r\n",
					"    try:\r\n",
					"        if col == 1 or col == \"1\":\r\n",
					"            return \"MONDAY\"\r\n",
					"        elif col == 2 or col == \"2\":\r\n",
					"            return \"TUESDAY\"\r\n",
					"        elif col == 3 or col == \"3\":\r\n",
					"            return \"WEDNSDAY\"\r\n",
					"        elif col == 4 or col == \"4\":\r\n",
					"            return \"THURSDAY\"\r\n",
					"        elif col == 5 or col == \"5\":\r\n",
					"            return \"FRIDAY\"\r\n",
					"        elif col == 6 or col == \"6\":\r\n",
					"            return \"SATURDAY\"\r\n",
					"        elif col == 7 or col == \"7\":\r\n",
					"            return \"SUNDAY\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getweekend(col):\r\n",
					"    try:\r\n",
					"        if col == 6 or col == \"6\" or col == 7 or col == \"7\":\r\n",
					"            return \"YES\"\r\n",
					"        else:\r\n",
					"            return \"NO\"\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"\r\n",
					"def getholiday(col):\r\n",
					"    try:\r\n",
					"        for d in holidays.UnitedStates(years=[2016,2017,1018,2019]).items():\r\n",
					"            if str(d[0]) == col:\r\n",
					"                return \"YES\"\r\n",
					"        return \"NO\"\r\n",
					"    except:\r\n",
					"        pass"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Customer Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"getBirthYear = udf(lambda z: getsubstring(z,0,4))\r\n",
					"getBirthMonth = udf(lambda z: getsubstring(z,5,7))\r\n",
					"getAge = udf(lambda z: calc_age(z))\r\n",
					"generationBucketizer = udf(lambda z: bucketize(z,[18,23,39,55,64,199],[\"Children\",\"GenerationZ\",\"Millenials\",\"GenX\",\"BabyBoomer\",\"Silent\"]),StringType())\r\n",
					"ageCategoryBucketizer = udf(lambda z: bucketize(z,[18,25,35,45,55,65,199],[\"0-18\",\"19-25\",\"26-35\",\"36-45\",\"46-55\",\"56-65\",\"66+\"]),StringType())\r\n",
					"socialStateBucketizer = udf(lambda z: social_state(z,[\"0-19999\",\"100000-199999\"],[\"Poor\",\"Wealthy\",\"Mid Level Income\"]),StringType())\r\n",
					"getPhoneOperator = udf(lambda z: getsubstring(z,0,3))\r\n",
					"\r\n",
					" #sk_generator('customer_id','sk_customer','customer')\\\r\n",
					"customer_df = customer_df\\\r\n",
					".withColumn( 'birth_year', getBirthYear('birth_date'))\\\r\n",
					".withColumn( 'birth_month', getBirthMonth('birth_date'))\\\r\n",
					".withColumn( 'age', getAge('birth_date'))\\\r\n",
					".withColumn( 'generation', generationBucketizer('age'))\\\r\n",
					".withColumn( 'age_category', ageCategoryBucketizer('age'))\\\r\n",
					".withColumn( 'social_state', socialStateBucketizer('income'))\\\r\n",
					".withColumn( 'phone_operator', getPhoneOperator('phone_no'))\\\r\n",
					".drop('phone_no')"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Delivery Services Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# delivery_services_df = sk_generator('delivery_service_id','sk_delivery_service','deliveryservices')\r\n",
					"\r\n",
					"# delivery_service_duration"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Supplier Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# supplier_df = sk_generator('supplier_id','sk_supplier','supplier')\r\n",
					"supplier_df = supplier_df.drop(\"supplied_quantity_for_month\")\r\n",
					"# supplied_quantity_for_month"
				],
				"execution_count": 25
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Seller Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"getSellerSize  = udf(lambda z: check_seller_size(z,[5,20,100]))\r\n",
					"getUniqueProductCat = udf(lambda z: get_unique_cat(z))\r\n",
					"\r\n",
					"seller_product_df = order_items_df.select(col(\"seller_id\"),col(\"product_id\"))\\\r\n",
					".groupBy(\"seller_id\").agg(count('product_id').alias('no_of_products_sold'))\r\n",
					"\r\n",
					"seller_product_cat_df = order_items_df.select(col(\"seller_id\"),col(\"product_id\"))\\\r\n",
					".join(products_df.select(col(\"product_id\"),col(\"product_category_name\")).distinct(), how=\"left\", on=\"product_id\")\\\r\n",
					".groupBy(\"seller_id\").agg(collect_list(\"product_category_name\"))\\\r\n",
					".withColumnRenamed('collect_list(product_category_name)','product_categories_sold')\\\r\n",
					".withColumn('product_categories_sold', getUniqueProductCat(col('product_categories_sold')))\r\n",
					"\r\n",
					"# sk_generator('seller_id','sk_seller','seller')\\\r\n",
					"seller_df = seller_df\\\r\n",
					".join(seller_product_df, how = \"left\", on=\"seller_id\")\\\r\n",
					".join(seller_product_cat_df, how = \"left\", on=\"seller_id\")\\\r\n",
					".withColumn('seller_size', getSellerSize(col('no_of_products_sold')))"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"GeoLocation Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"geolocation_df = geolocation_df\\\r\n",
					".withColumnRenamed('geolocation_lat','geolocation_lattitude')\\\r\n",
					".withColumnRenamed('geolocation_lng','geolocation_longtitude')"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Order Details Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"getHasDiscount = udf(lambda z: check_discount(z))\r\n",
					"getIsDeliveryDelayed = udf(lambda z: chekc_delayed_delivery(z))\r\n",
					"\r\n",
					"order_discount_df = order_items_df.select(col(\"order_id\"),col(\"product_id\")).distinct()\\\r\n",
					".join(products_df.select(\"product_id\",\"product_discount\"), how = \"left\", on =\"product_id\")\\\r\n",
					".groupBy(\"order_id\").agg({\"product_discount\":\"sum\"})\r\n",
					"\r\n",
					"# order_df.select(\"order_id\",\"order_status\",\"delivery_service_id\",\"order_delivered_customer_date\",\"order_estimated_delivery_date\")\\\r\n",
					"# .createOrReplaceTempView(\"orderdetails\")\r\n",
					"#  sk_generator('order_id','sk_order_details','orderdetails')\\\r\n",
					"\r\n",
					"order_details_df = order_df.select(\"order_id\",\"order_status\",\"delivery_service_id\",\"order_delivered_customer_date\",\"order_estimated_delivery_date\")\\\r\n",
					".join(order_payments_df.select(col(\"order_id\"), col(\"payment_type\")).distinct(), how=\"left\", on = \"order_id\")\\\r\n",
					".join(order_discount_df, how=\"left\", on = \"order_id\")\\\r\n",
					".withColumn(\"is_returned\",lit(\"NO\"))\\\r\n",
					".withColumn(\"has_discount\",getHasDiscount(\"sum(product_discount)\"))\\\r\n",
					".withColumn(\"is_delivery_delayed\",getIsDeliveryDelayed(array(\"order_delivered_customer_date\",\"order_estimated_delivery_date\")))\\\r\n",
					".drop(\"order_delivered_customer_date\",\"order_estimated_delivery_date\",\"sum(product_discount)\")\r\n",
					"\r\n",
					"# order_df --> FACT\r\n",
					"# order_purchase_timestamp\r\n",
					"# order_approved_at\r\n",
					"# order_delivered_carrier_date\r\n",
					"# order_delivered_customer_date\r\n",
					"# order_estimated_delivery_date\r\n",
					"\r\n",
					"# order_items_df -> FACt\r\n",
					"# order_id\r\n",
					"# order_item_id\r\n",
					"# shipping_limit_date\r\n",
					"# price\r\n",
					"# freight_value\r\n",
					"\r\n",
					"# order_payments_df -> FACT\r\n",
					"# payment_installments\r\n",
					"# payment_value\r\n",
					"# payment_sequential\r\n",
					"\r\n",
					"# Format dates to remove .00000 part in FACT"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Review Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_details(col):\r\n",
					"    try:\r\n",
					"        return str(col).replace(\",\",\"\")\r\n",
					"    except:\r\n",
					"        pass\r\n",
					"\r\n",
					"getReviewSentiment = udf(lambda z: get_review_sentiment(z))\r\n",
					"cleanDetails = udf(lambda z: clean_details(z))\r\n",
					"\r\n",
					"# sk_generator('review_id','sk_review','orderreviews')\\\r\n",
					"order_reviews_df = order_reviews_df\\\r\n",
					".select(\"review_id\",\"order_id\",\"review_score\",\"review_comment_title\",\"review_comment_message\",\"review_creation_date\",\"review_answer_timestamp\")\\\r\n",
					".withColumn('review_sentiment', getReviewSentiment(col('review_score')))\\\r\n",
					".withColumn('review_comment_title', cleanDetails(col('review_comment_title')))\\\r\n",
					".withColumn('review_comment_message', cleanDetails(col('review_comment_message')))"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Product Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"getProductVolume = udf(lambda z: get_product_volume(z))\r\n",
					"getPackageType = udf(lambda z: get_package_type(z))\r\n",
					"\r\n",
					"# sk_generator('product_id','sk_product','product')\\\r\n",
					"products_df = products_df\\\r\n",
					".withColumn('product_volume', getProductVolume(array('product_length_cm','product_height_cm','product_width_cm')))\\\r\n",
					".withColumn('package_type', getPackageType(array('product_volume','product_weight_g')))\\\r\n",
					".withColumnRenamed('product_photos_qty','product_photos_quantity')\\\r\n",
					".drop('supplier_id')\r\n",
					"\r\n",
					"# unit_price\r\n",
					"# product_quantity_on_hand\r\n",
					"# product_discount"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Date Dimension"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sdate = datetime.date(2016,1,1)   # start date\r\n",
					"edate = datetime.date(2019,12,31)   # end date\r\n",
					"date_list = pandas.date_range(sdate,edate-datetime.timedelta(days=1),freq='d')\r\n",
					"dim_finalized_list = []\r\n",
					"for i in range(0,len(date_list)):\r\n",
					"    dim_finalized_list.append((i+1,str(date_list[i])))\r\n",
					"\r\n",
					"getDate =  udf(lambda z: getsubstring(z,0,10))\r\n",
					"getYear = udf(lambda z: getsubstring(z,0,4))\r\n",
					"getMonth = udf(lambda z: getsubstring(z,5,7))\r\n",
					"getDayofMonth = udf(lambda z: getsubstring(z,8,10))\r\n",
					"getQuarter = udf(lambda z: getquarter(z,5,7))\r\n",
					"getWeekDay = udf(lambda z: getweekday(z))\r\n",
					"getWeekYear = udf(lambda z: getweekyear(z))\r\n",
					"getLeapYear = udf(lambda z: getleapyear(z))\r\n",
					"getDayPrefix = udf(lambda z: getdayprefix(z))\r\n",
					"getWeekend = udf(lambda z: getweekend(z))\r\n",
					"getHoliday = udf(lambda z: getholiday(z))\r\n",
					"\r\n",
					"dim_date_df = spark.createDataFrame(data=dim_finalized_list, schema = [\"sk_date\",\"date\"])\\\r\n",
					".withColumn(\"date\",getDate(\"date\"))\\\r\n",
					".withColumn(\"year\",getYear(\"date\"))\\\r\n",
					".withColumn(\"month\",getMonth(\"date\"))\\\r\n",
					".withColumn(\"quarter\",getQuarter(\"date\"))\\\r\n",
					".withColumn(\"day_of_month\",getDayofMonth(\"date\"))\\\r\n",
					".withColumn(\"day_of_week\",getWeekDay(\"date\"))\\\r\n",
					".withColumn(\"week_of_year\",getWeekYear(\"date\"))\\\r\n",
					".withColumn(\"is_leap_year\",getLeapYear(\"date\"))\\\r\n",
					".withColumn(\"is_weekend\",getWeekend(\"day_of_week\"))\\\r\n",
					".withColumn(\"day_prefix\",getDayPrefix(\"day_of_week\"))\\\r\n",
					".withColumn(\"is_holiday\",getHoliday(\"date\"))\\\r\n",
					".drop(\"sk_date\") # dropping temp SK"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Processed Data Dumping"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set(\"spark.sql.broadcastTimeout\", \"36000\")\r\n",
					"\r\n",
					"customer_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_customer.csv\")\r\n",
					"delivery_services_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_delivery_service.csv\")\r\n",
					"supplier_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_supplier.csv\")\r\n",
					"seller_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_seller.csv\")\r\n",
					"product_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_product.csv\")\r\n",
					"order_details_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_order_details.csv\")\r\n",
					"order_review_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_order_review.csv\")\r\n",
					"geolocation_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_geolocation.csv\")\r\n",
					"date_processed_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name,account_name,\"data/curated/dim_date.csv\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"customer_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(customer_processed_path)\r\n",
					"\r\n",
					"delivery_services_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(delivery_services_processed_path)\r\n",
					"\r\n",
					"supplier_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(supplier_processed_path)\r\n",
					"\r\n",
					"seller_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(seller_processed_path)\r\n",
					"\r\n",
					"order_details_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(order_details_processed_path)\r\n",
					"\r\n",
					"order_reviews_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(order_review_processed_path)\r\n",
					"\r\n",
					"products_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(product_processed_path)\r\n",
					"\r\n",
					"geolocation_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(geolocation_processed_path)\r\n",
					"\r\n",
					"dim_date_df.coalesce(1).write\\\r\n",
					".format(\"csv\")\\\r\n",
					".mode(\"overwrite\")\\\r\n",
					".option(\"header\", \"true\")\\\r\n",
					".save(date_processed_path)"
				],
				"execution_count": 15
			}
		]
	}
}